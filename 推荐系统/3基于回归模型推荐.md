# 基于回归模型的协同过滤

## 目录

1. 回归模型
2. 随机梯度下降优化
3. 交替最小二乘法优化



## 回归模型

如果我们将评分矩阵看作是三维空间中的一个连续平面，那么就可以借助线性回归思想来预测目标用户对目标商品的评分。其中一种实现策略被称为Baseline

![偏置](偏置.png)

Baseline的思想基于以下的假设：

* 评分矩阵的全局平均值为$\mu$
* 每一个用户都有相对于全局的评分偏好$b_{u}$
* 每一个商品都有相对于全局的评分偏好$b_i$

则任意用户对商品的评分为：
$$
\hat{r}_{ui} = b_{ui} = \mu + b_u + b_i
$$

利用平方差构建损失函数如下：
$$
\begin{split}
Cost &= \sum_{u,i\in R}(r_{ui}-\hat{r}_{ui})^2
\\&=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2
\end{split}
$$
加入L2正则化
$$
Cost=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)
$$


Baseline的目标在于要得出最优的$b_u$和 $b_i$，使得模型有较好的数据拟合效果，即在训练集和测试集上的损失值最小。对于优化过程的求解，我们一般采用**随机梯度下降算法**或者**交替最小二乘法**来实现。



## 随机梯度下降优化

### a) 算法思路

梯度下降法(BGD)

* 构建损失函数

$$
\begin{split}
&J(\theta)=Cost=f(b_u, b_i)\\
\\
&J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)
\end{split}
$$

* 损失函数求偏导

$$
\begin{split}
\cfrac{\partial}{\partial b_u} J(\theta)&=\cfrac{\partial}{\partial b_u} f(b_u, b_i)
\\&=2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)(-1) + 2\lambda{b_u}
\\&=-2\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambda*b_u
\end{split}
$$

* $b_u$更新

$$
\begin{split}
b_u&:=b_u - \alpha*(-\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + \lambda * b_u)\\
&:=b_u + \alpha*(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) - \lambda* b_u)
\end{split}
$$

* $b_i$更新

$$
b_i:=b_i + \alpha*(\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) -\lambda*b_i)
$$



随机梯度下降(SGD)

* 单样本损失

$$
\begin{split}
error &= 1/2 *(r_{ui}-\mu-b_u-b_i)^2 + 1/2 * \lambda * (b^2_u + b^2_i)
\end{split}
$$

* 参数更新

$$
\begin{split}
b_u&:=b_u + \alpha*((r_{ui}-\mu-b_u-b_i) -\lambda*b_u)  \\
&:=b_u + \alpha*(error - \lambda*b_u) \\
\\
b_i&:=b_i + \alpha*((r_{ui}-\mu-b_u-b_i) -\lambda*b_i)\\
&:=b_i + \alpha*(error -\lambda*b_i)
\end{split}
$$



### b) 算法实现

```python
# 构建模型对象
def __init__(self, number_epochs, alpha, reg, columns=["uid", "iid", "rating"])
# 配置模型
def fit(self, dataset)
# 利用随机梯度下降训练模型参数
def sgd(self)
# 利用模型预测目标用户对目标商品的评分
def predict(self, uid, iid)
# 利用模型预测测试集中的所有目标评分
def test(slef, testset)
```



### c) 效果评估

```python
# 返回经过切分的评分矩阵，为了保证用户数量不变，将每个用户的评分数据按比例进行拆分
def data_split(data_path, x=0.8, random=False)
# 返回准确性指标的评估结果
def accuray(predict_result, methond="rmse")
```



### d) 评估结果

```
rmse:  0.9375 mae:  0.7232
```



## 交替最小二乘法优化

### a) 算法思路

最小二乘法

* 构建损失函数
  $$
  J(\theta)=\sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i)^2 + \lambda*(\sum_u {b_u}^2 + \sum_i {b_i}^2)
  $$

* 损失函数求偏导
  $$
  \cfrac{\partial}{\partial b_u} f(b_u, b_i) =-2 \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) + 2\lambda * b_u
  $$

* 

* 令偏导为0
  $$
  \sum_{u,i\in R}(r_{ui}-\mu-b_u-b_i) = \lambda* b_u
  \\\sum_{u,i\in R}(r_{ui}-\mu-b_i) = \sum_{u,i\in R} b_u+\lambda * b_u
  $$

* 简化公式，令$\sum_{u,i\in R} b_u \approx |R(u)|*b_u$，即假设每一项的偏置都相等，其中$|R(u)|$表示用户$u$的有过评分数量。

* 求$b_u$
  $$
  b_u := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_i)}{\lambda_1 + |R(u)|}
  $$

* 求$b_i$
  $$
  b_i := \cfrac {\sum_{u,i\in R}(r_{ui}-\mu-b_u)}{\lambda_2 + |R(i)|}
  $$



交替最小二乘法(ALS)

* $b_u$和$b_i$的表达式各自包含对方，无法直接求解

* 求$b_u$时，将$b_i$看作是已知；求$b_i$时，将$b_u$看作是已知；如此反复交替，不断更新两者的值，求得最终的结果。



### b) 算法实现

```python
# 构建模型对象
def __init__(self, number_epochs, reg_bu, reg_bi, columns=["uid", "iid", "rating"])
# 配置模型
def fit(self, dataset)
# 利用交替最小二乘法训练模型参数
def als(self)
# 利用模型预测目标用户对目标商品的评分
def predict(self, uid, iid)
# 利用模型预测测试集中的所有目标评分
def test(slef, testset)
```



### c) 效果评估

```python
# 返回经过切分的评分矩阵，为了保证用户数量不变，将每个用户的评分数据按比例进行拆分
def data_split(data_path, x=0.8, random=False)
# 返回准确性指标的评估结果
def accuray(predict_result, methond="rmse")
```



### d) 评估结果

```
rmse:  0.8663 mae:  0.6695
```

